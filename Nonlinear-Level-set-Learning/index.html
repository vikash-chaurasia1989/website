<!doctype html>
<html lang="en" class="no-js">

<head>
    <meta charset="utf-8"> <!-- begin SEO -->
    <title>Learning the Structure of Level Sets from Sparse Data - Vikash Chaurasia</title>
    <meta property="og:locale" content="en-US">
    <meta property="og:site_name" content="Vikash Chaurasia">
    <meta property="og:title" content="Learning the Structure of Level Sets from Sparse Data">
    <link rel="canonical" href="index.html">
    <meta property="og:url" content="https://agrubertx.github.io/Nonlinear-Level-set-Learning/">
    <meta property="og:description" content="Nonlinear Level set Learning">
    <script
        type="application/ld+json"> { "@context" : "http://schema.org", "@type" : "Person", "name" : "Vikash Chaurasia", "url" : "https://agrubertx.github.io", "sameAs" : null } </script>
    <!-- end SEO -->
    <link href="../feed.xml" type="application/atom+xml" rel="alternate" title="Vikash Chaurasia Feed">
    <!-- http://t.co/dKP3o1e -->
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script> document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js '; </script>
    <!-- For all browsers -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <meta http-equiv="cleartype" content="on"> <!-- start custom head snippets -->
    <link rel="apple-touch-icon" sizes="57x57"
        href="https://agrubertx.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
    <link rel="apple-touch-icon" sizes="60x60"
        href="https://agrubertx.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
    <link rel="apple-touch-icon" sizes="72x72"
        href="https://agrubertx.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
    <link rel="apple-touch-icon" sizes="76x76"
        href="https://agrubertx.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
    <link rel="apple-touch-icon" sizes="114x114"
        href="https://agrubertx.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
    <link rel="apple-touch-icon" sizes="120x120"
        href="https://agrubertx.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
    <link rel="apple-touch-icon" sizes="144x144"
        href="https://agrubertx.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
    <link rel="apple-touch-icon" sizes="152x152"
        href="https://agrubertx.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
    <link rel="apple-touch-icon" sizes="180x180"
        href="https://agrubertx.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
    <link rel="icon" type="image/png" href="https://agrubertx.github.io/images/favicon-32x32.png?v=M44lzPylqQ"
        sizes="32x32">
    <link rel="icon" type="image/png" href="https://agrubertx.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ"
        sizes="192x192">
    <link rel="icon" type="image/png" href="https://agrubertx.github.io/images/favicon-96x96.png?v=M44lzPylqQ"
        sizes="96x96">
    <link rel="icon" type="image/png" href="https://agrubertx.github.io/images/favicon-16x16.png?v=M44lzPylqQ"
        sizes="16x16">
    <link rel="manifest" href="https://agrubertx.github.io/images/manifest.json?v=M44lzPylqQ">
    <link rel="mask-icon" href="https://agrubertx.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
    <link rel="shortcut icon" href="https://agrubertx.github.io/images/favicon.ico?v=M44lzPylqQ">
    <meta name="msapplication-TileColor" content="#000000">
    <meta name="msapplication-TileImage" content="https://agrubertx.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
    <meta name="msapplication-config" content="https://agrubertx.github.io/images/browserconfig.xml?v=M44lzPylqQ">
    <meta name="theme-color" content="#ffffff">
    <link rel="stylesheet" href="../assets/css/academicons.css" />
    <script
        type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
    <script
        type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } }); </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
        type="text/javascript"></script> <!-- end custom head snippets -->
</head>

<body>
    <!--[if lt IE 9]><div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->
    <div class="masthead">
        <div class="masthead__inner-wrap">
            <div class="masthead__menu">
                <nav id="site-nav" class="greedy-nav"> <button>
                        <div class="navicon"></div>
                    </button>
                    <ul class="visible-links">
                        <li class="masthead__menu-item masthead__menu-item--lg"><a href="../index.html">Vikash Chaurasia</a></li>
                        <li class="masthead__menu-item"><a href="../publications/index.html">Publications</a></li>
                        <li class="masthead__menu-item"><a href="../talks/index.html">Talks</a></li>
                        <li class="masthead__menu-item"><a href="../teaching/index.html">Teaching</a></li>
                        <li class="masthead__menu-item"><a href="../code.html">Code</a></li>
                        <li class="masthead__menu-item"><a href="../cv/index.html">CV</a></li>
                        <li class="masthead__menu-item"><a href="../gallery/index.html">Gallery</a></li>
                    </ul>
                    <ul class="hidden-links hidden"></ul>
                </nav>
            </div>
        </div>
    </div>
    <div id="main" role="main">
        <div class="sidebar sticky">
            <div itemscope itemtype="http://schema.org/Person">
                <div class="author__avatar"> <img src="../images/profile2023-2.jpg" class="author__avatar"
                        alt="Dr. Vikash Chaurasia"></div>
                <div class="author__content">
                    <h3 class="author__name">Dr. Vikash Chaurasia</h3>
                    <p class="author__bio">Postdoctoral Researcher, Okinawa Institute of Science and Technology</p>
                </div>
                <div class="author__urls-wrapper"> <button class="btn btn--inverse">Follow</button>
                    <ul class="author__urls social-icons">
                        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Albuquerque, NM</li>
                        <li><a href="https://scholar.google.com/citations?user=CJVuqfoAAAAJ&hl=en"><i
                                    class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
                        <li><a href="https://github.com/agrubertx"><i class="fab fa-fw fa-github"
                                    aria-hidden="true"></i> Github</a></li>
                        <li><a href="https://orcid.org/0000-0001-7107-5307"><i class="ai ai-orcid-square ai-fw"></i>
                                ORCID</a></li>
                        <li><a href="https://www.researchgate.net/profile/Anthony_Gruber2"><i
                                    class="fab fa-fw fa-researchgate" aria-hidden="true"></i> ResearchGate</a></li>
                    </ul>
                </div>
            </div>
        </div>
        <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
            <meta itemprop="headline" content="Learning the Structure of Level Sets from Sparse Data">
            <meta itemprop="description" content="Nonlinear Level set Learning">
            <div class="page__inner-wrap">
                <header>
                    <h1 class="page__title" itemprop="headline">Learning the Structure of Level Sets from Sparse Data
                    </h1>
                </header>
                <section class="page__content" itemprop="text">
                    <!-- <script src="scripts/load-mathjax.js" async></script> -->
                    <p><img src="../images/nllfront.png" alt="image-center" class="align-center" />
                        \(\newcommand{\bb}[1]{\mathbf{#1}} \newcommand{\nn}[1]{\left\|#1\right\|}
                        \newcommand{\mmu}{\bm{\mu}} \newcommand{\IP}[3]{\left\langle #2, #3\right\rangle_{#1}}
                        \newcommand{\kdf}{\mathrm{ker}\,f'} \newcommand{\idf}{\mathrm{im}\,f'}
                        \DeclareMathOperator*{\argmin}{arg\,min}\)</p>
                    <p>let \(U \subset \mathbb{R}^n\) be a bounded domain and consider the problem of approximating a
                        continuously differentiable function \(f:U \to \mathbb{R}\) based on some predefined samples
                        \(\{ \mathbf{x}^s, f(\mathbf{x}^s), \nabla f(\mathbf{x}^s) \}_{s\in S}\) of the function and its
                        gradient vector field. To generate a pointwise approximation to $f$, it is reasonable to seek a
                        regression approximant \(\tilde{f}:U\to\mathbb{R}\) which satisfies the minimization condition
                    </p>\[\tilde{f}(\bb{x}) \in \argmin_{g\in C^1(U)} \nn{f(\bb{x}) - g(\bb{x})}_2^2. \label{eq:reg}\]
                    <p>However, if the dimension \(n\) is large relative to the number \(\lvert S \rvert\) of available
                        samples (i.e. sparse data), training a model to approximate \(f\) directly becomes infeasible.
                        Indeed, unless the training data itself has a hidden low-dimensional structure, popular
                        unsupervised learning methods such as feed-forward neural networks are prone to overfitting,
                        leading to poor accuracy on new data as a result of inadequate generalizability.</p>
                    <p>The Nonlinear Level set Learning (NLL) method proposed by Zhang et al. aims to circumvent this
                        issue by nonlinearly ‘projecting’ the input data into a low-dimensional space where the function
                        \(f\) changes the most on average. This can be thought of as follows: consider computing a
                        diffeomorphism (differentiable bijection with differentiable inverse) \(\bb{g}:\mathbb{R}^n \to
                        \mathbb{R}^n\), \(\bb{z} = \bb{g}(\bb{x})\) and \(\bb{h}\circ\bb{g}\) is the identity on
                        \(\mathbb{R}^n\), which separates the domain of the push-forward function \(f\circ \bb{h}\) into
                        global pairs \(\bb{z} = (\bb{z}_A, \bb{z}_I)\) of “active” and “inactive” coordinates. If
                        \(\bb{g},\bb{h}\) can be constructed such that the sensitivity of \(f\circ\bb{h}\) to the
                        coordinates \(\bb{z}_I\) is sufficiently low, then it is reasonable to conclude that the
                        transformed data \(\bb{g}(\bb{x})\) lies in a lower-dimensional submanifold of the ambient space
                        \(\mathbb{R}^n\), and for any inactive coordinate \(z^i\in\bb{z}_I\) the domain of
                        \(f\circ\bb{h}\) can be restricted to \(\mathrm{Span}\{z^i\}^\perp\) with negligible impact on
                        the function value. Therefore, regression can be applied to obtain a lower-dimensional mapping
                        \(\hat{f}:\mathbb{R}^{\lvert A \rvert} \to \mathbb{R}\) such that \(f(\bb{x}) \approx
                        \hat{f}(\bb{z}_A)\). Writing \(\bb{z}_A=:\bb{g}_A(\bb{x})\) to denote the first \(\lvert A
                        \rvert\) components of the function \(\bb{g}\), this means computing a generalized ridge
                        function</p>\[\hat{f}(\bb{g}_A(\bb{x})) \in \argmin_{\varphi:\mathbb{R}^{|A|}\to\mathbb{R}}
                    \nn{f(\bb{x}) - \varphi(\bb{g}_A(\bb{x}))}_2^2, \label{eq:nllreg}\]<p>where \(\bb{g}\) acts as the
                        (nonlinear) projection operator. Once \(\bb{g}\) is obtained, computing the required \(\hat{f}\)
                        in \eqref{eq:nllreg} is automatically a more feasible regression problem than \eqref{eq:reg},
                        since almost all of the variation in \(f\) has been concentrated in the lower dimensional image
                        $\bb{z}_A$ where the data is automatically more dense. Most importantly, the necessary
                        projection from \(\bb{z}\) to \(\bb{z}_A\) is simple and canonical: after truncating the domain
                        of \(\bb{h}\) by the span of the inactive variables \(\bb{z}_I\), the active variables \(\{
                        \bb{z}_A \}\) parameterize the low-dimensional inputs by definition.</p>
                    <p>Computing this mapping in our case is done by minimizing an inspired loss functional in the
                        context of a particular “reversible neural network” (RevNet) architecture (also employed in the
                        original NLL paper). The key ingredients to this are the guaranteed invertibility of the RevNet
                        along with the coercivity of the loss. In particular, we minimize</p>\[L(\bb{h}) =
                    \frac{1}{|S|}\sum_{s\in S} \nn{(f\circ\bb{h})'(\bb{z}^s)}^2_\perp, \label{nllloss3}\]<p>where
                        \(\nn{\cdot}_\perp\) denotes the norm on the subspace orthogonal to the active direction
                        \(\bb{e}_1\) at each point, i.e. the trace of the quadratic form \(\sqrt{\IP{}{\cdot}{\cdot}}\)
                        with respect to the basis \(\{ \bb{e}_2,...,\bb{e}_n \}\). This encourages the algorithm to find
                        a mapping \(\bb{h}\) which reduces the composite function \(f\circ\bb{h}\) to a function of one
                        variable \(\bb{z}_1 = \bb{g}_1(\bb{z})\). Note that (under some assumptions on \(f\)) the loss
                        \(L\) is coercive on the complement of \(\bb{z}_1\), allowing the network to find something
                        which globally approximates the local mapping guaranteed by the Implicit Function Theorem in a
                        neighborhood of each point. Experiments show that this is quite successful, producing results
                        which substantially improve over those produced by the original NLL algorithm.</p>
                    <p>A (mostly) clean implementation of our algorithm can be found on my <a href="../code.html">code
                            page</a>.</p>
                    <h1 id="relevant-publications">Relevant Publications</h1>
                    <p>(NLL2) <b>Vikash Chaurasia</b>, Max Gunzburger, Lili Ju, Yuankai Teng, Zhu Wang. <i>Nonlinear Level
                            Set Learning for Function Approximation on Sparse Data with Applications to Parametric
                            Differential Equations.</i> Preprint version <a
                            href="https://arxiv.org/pdf/2104.14072.pdf">available here.</a></p>
                    <p>(NLL) Guannan Zhang, Jiaxin Zhang, Jacob Hinkle. <i>Learning nonlinear level sets for
                            dimensionality reduction in function approximation.</i> Advances in Neural Information
                        Processing Systems, Vol. 32, Curran Associates, Inc., 2019. <a
                            href="https://proceedings.neurips.cc/paper/2019/file/464074179972cbbd75a39abc6954cd12-Paper.pdf">Available
                            here</a></p>
                </section>
                <footer class="page__meta"></footer>
            </div>
        </article>
    </div>
    <div class="page__footer">
        <footer> <!-- start custom footer snippets --> <!-- <a href="/sitemap/">Sitemap</a> -->
            <!-- end custom footer snippets -->
            <div class="page__footer-follow">
                <ul class="social-icons">
                    <li><strong>Follow:</strong></li>
                    <li><a href="http://github.com/agrubertx"><i class="fab fa-github" aria-hidden="true"></i>
                            GitHub</a></li>
                    <li><a href="../feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
                </ul>
            </div>
            <div class="page__footer-copyright">&copy; 2023 Vikash Chaurasia. Powered by <a href="http://jekyllrb.com"
                    rel="nofollow">Jekyll</a> &amp; <a
                    href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a
                    href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal
                    Mistakes</a>.</div>
        </footer>
    </div>
    <script src="../assets/js/main.min.js"></script>
    <script> (function (i, s, o, g, r, a, m) { i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () { (i[r].q = i[r].q || []).push(arguments) }, i[r].l = 1 * new Date(); a = s.createElement(o), m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m) })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"> </script>
    <script
        type="text/x-mathjax-config"> MathJax.Hub.Config({ extensions: ["tex2jax.js"], jax: ["input/TeX", "output/HTML-CSS"], tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true }, "HTML-CSS": { availableFonts: ["TeX"] } }); </script>
</body>

</html>